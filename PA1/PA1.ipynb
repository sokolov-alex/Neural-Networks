{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "__author__ = 'alex'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from time import clock\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import jit\n",
    "#%matplotlib inline\n",
    "\n",
    "def plot_digit(idx):\n",
    "    img = X_train[idx]\n",
    "    if img.shape[0]>784:\n",
    "        img = img[:-1]\n",
    "    img = np.reshape(img, (28,28))\n",
    "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
    "    plt.title('true label: %d' % y_train[idx])\n",
    "    plt.show()\n",
    "\n",
    "data = np.load('../Data/mnist.npz')\n",
    "\n",
    "X_train = data['images_train']/255\n",
    "y_train = data['labels_train']\n",
    "X_test = data['images_test']/255\n",
    "y_test = data['labels_test']\n",
    "\n",
    "def add_bias(x):\n",
    "    ones = np.ones((x.shape[0],1))\n",
    "    return np.hstack((x, ones))\n",
    "\n",
    "def shuffle_data(X,y):\n",
    "    idx = np.random.permutation(len(X))\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def zscore(X):\n",
    "    return (X - X.mean(axis=1).reshape(len(X),1))/X.std(axis=1).reshape(len(X),1)\n",
    "\n",
    "def PCA(X):\n",
    "    pass\n",
    "    #PCA - leave 185 dimensions\n",
    "\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "X_train = add_bias(X_train)\n",
    "X_test = add_bias(X_test)\n",
    "\n",
    "X_train = zscore(X_train)\n",
    "X_test = zscore(X_test)\n",
    "\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, dim):\n",
    "        self.w = np.zeros(dim)\n",
    "        self.eta = 10e-7\n",
    "\n",
    "    def Gw(self, x):\n",
    "        return sigmoid(x.dot(self.w))\n",
    "\n",
    "    def cost(self, y_true, y_pred):\n",
    "        return -(y_true * np.ma.log(y_pred).data + (1-y_true) * np.ma.log(1-y_pred).data).sum()\n",
    "\n",
    "    def gradient(self, y_true, y_pred, X):\n",
    "        return -X * (y_true - y_pred).reshape(y_true.shape[0],1)\n",
    "\n",
    "    def SGD(self, y_true, y_pred, X):\n",
    "        self.w = self.w - self.eta * self.gradient(y_true,y_pred,X).sum(axis=0)\n",
    "\n",
    "    def fit(self, X_train, y_train, mini_batch = 100, verbose = 10):\n",
    "        old_loss = np.inf\n",
    "        of = 0\n",
    "        for i in range(15):\n",
    "            X_train, y_train = shuffle_data(X_train, y_train)\n",
    "            for b in range(mini_batch, X_train.shape[0], mini_batch):\n",
    "                x_batch, y_batch = X_train[b-mini_batch:b], y_train[b-mini_batch:b]\n",
    "                y_pred = self.predict(x_batch)\n",
    "                self.SGD(y_batch, y_pred, x_batch)\n",
    "\n",
    "            if i % verbose == 0:\n",
    "                loss = self.cost(y_batch, y_pred)\n",
    "                if old_loss - loss >= 0:\n",
    "                    of -= 1\n",
    "                    self.eta *= 2\n",
    "                else:\n",
    "                    of += 1\n",
    "                    self.eta /= 3\n",
    "                    if of > 5:\n",
    "                        self.eta /= 20\n",
    "                        if of > 10:\n",
    "                            break\n",
    "                print 'rate: ', self.eta\n",
    "                old_loss = loss\n",
    "\n",
    "                print 'loss: ', loss\n",
    "            if loss < 0.001:\n",
    "                print 'enough (loss = 0.01)'\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.Gw(X)\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        y_pred = (self.predict(X) >= 0.5).astype(int)\n",
    "        #p = pd.DataFrame(y_pred.T)\n",
    "        #p[1] = y_train.T\n",
    "        #p[:100].plot()\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "class SoftmaxRegression(LogisticRegression):\n",
    "    def __init__(self, dim):\n",
    "        LogisticRegression.__init__(self, dim)\n",
    "\n",
    "    def Gw(self, x):\n",
    "        return np.exp(x.dot(self.w))/np.exp(x.dot(self.w)).sum(axis=1).reshape((100,1))\n",
    "\n",
    "    def cost(self, y_true, y_pred):\n",
    "        return -(y_true * np.ma.log(y_pred).data).sum().sum()\n",
    "\n",
    "    def SGD(self, y_true, y_pred, X):\n",
    "        self.w = self.w - self.eta * self.gradient(y_true,y_pred,X).sum(axis=1).T\n",
    "\n",
    "    def gradient(self, y_true, y_pred, X):\n",
    "        return -X.reshape((1, X.shape[0], X.shape[1])).repeat(10,0) * (y_true - y_pred).T.reshape(y_true.shape[1],y_true.shape[0],1)\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    \n",
    "@jit\n",
    "def run_lr():\n",
    "    dim = X_train.shape[1]\n",
    "\n",
    "    for i in range(10):\n",
    "        lr = LogisticRegression(dim)\n",
    "        y_true = (y_train == i).astype(int)\n",
    "\n",
    "        lr.fit(X_train, y_true)\n",
    "\n",
    "        y_true_test = (y_test == i).astype(int)\n",
    "        print 'test accuracy: ', lr.score(X_test, y_true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate:  2e-06\n",
      "loss:  175.01295073\n",
      "rate:  4e-06\n",
      "loss:  119.881747458\n",
      "rate:  8e-06\n",
      "loss:  88.736968331\n",
      "rate:  1.6e-05\n",
      "loss:  52.6024796832\n",
      "rate:  3.2e-05\n",
      "loss:  50.8145692304\n",
      "rate:  6.4e-05\n",
      "loss:  35.540632843\n",
      "rate:  0.000128\n",
      "loss:  25.9173380977\n",
      "rate:  0.000256\n",
      "loss:  21.0566559244\n",
      "rate:  8.53333333333e-05\n",
      "loss:  40.9875989078\n",
      "rate:  0.000170666666667\n",
      "loss:  22.2383593884\n",
      "rate:  5.68888888889e-05\n",
      "loss:  34.4617850756\n",
      "rate:  0.000113777777778\n",
      "loss:  29.3151847278\n",
      "rate:  0.000227555555556\n",
      "loss:  24.7824094775\n",
      "rate:  7.58518518519e-05\n",
      "loss:  30.5033038459\n",
      "rate:  0.000151703703704\n",
      "loss:  26.0846219414\n",
      "test accuracy: "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1b1c5653c20b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoftmaxRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m'test accuracy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'total time: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-60c49b23eda9>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y_true)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-60c49b23eda9>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-60c49b23eda9>\u001b[0m in \u001b[0;36mGw\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mGw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "t1 = clock()\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "dim = (X_train.shape[1], n_classes)\n",
    "lb = LabelBinarizer()\n",
    "y_true = lb.fit_transform(y_train)\n",
    "y_true_test = lb.fit_transform(y_test)\n",
    "\n",
    "sr = SoftmaxRegression(dim)\n",
    "sr.fit(X_train, y_true, verbose=1)\n",
    "print 'test accuracy: ', sr.score(X_test, y_true_test)\n",
    "print 'total time: ', clock()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'test accuracy: ', sr.score(X_test, y_true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 18.55 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100000 loops, best of 3: 3.98 µs per loop\n",
      "100000 loops, best of 3: 3.98 µs per loop\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def test1():\n",
    "    a = np.ones((100,100))\n",
    "    b = a\n",
    "    c = b\n",
    "    d = c\n",
    "    return d\n",
    "\n",
    "def test2():\n",
    "    return np.ones((100,100))\n",
    "\n",
    "%timeit test1()\n",
    "%timeit test2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
